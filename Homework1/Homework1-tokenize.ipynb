{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Homework 1\n",
    "\n",
    "### IMDB Movie Review Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "First, Load libaries and read in datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"aclImdb\"\n",
    "train_p_name = os.listdir(filepath + \"/train/pos\")\n",
    "train_n_name = os.listdir(filepath + \"/train/neg\")\n",
    "test_p_name = os.listdir(filepath + \"/test/pos\")\n",
    "test_n_name = os.listdir(filepath + \"/test/neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxtfile(filepath, filenames):\n",
    "    filelist = []\n",
    "    for i in filenames:\n",
    "        file = open(filepath + i, \"r\")\n",
    "        filelist.append(file.read())\n",
    "        file.close()\n",
    "    return filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = readtxtfile(filepath + \"/train/pos/\", train_p_name)\n",
    "train_neg = readtxtfile(filepath + \"/train/neg/\", train_n_name)\n",
    "test_pos = readtxtfile(filepath + \"/test/pos/\", test_p_name)\n",
    "test_neg = readtxtfile(filepath + \"/test/neg/\", test_n_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_p = [1] * len(train_pos)\n",
    "train_label_n = [0] * len(train_neg)\n",
    "test_label_p = [1] * len(test_pos)\n",
    "test_label_n = [0] * len(test_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split train data into train set and validation set. Train dataset has 10,000 positive reviews and 10,000 negative reviews. Test data has 2,500 each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 10000\n",
    "\n",
    "train_data_p = train_pos[:train_split]\n",
    "train_data_p_label = train_label_p[:train_split]\n",
    "train_data_n = train_neg[:train_split]\n",
    "train_data_n_label = train_label_n[:train_split]\n",
    "\n",
    "\n",
    "val_data_p = train_pos[train_split:]\n",
    "val_data_p_label = train_label_p[train_split:]\n",
    "val_data_n = train_neg[train_split:]\n",
    "val_data_n_label = train_label_n[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_p + train_data_n\n",
    "train_label = train_data_p_label + train_data_n_label\n",
    "val_data = val_data_p + val_data_n\n",
    "val_label = val_data_p_label + val_data_n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV\\'s \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina\\'s pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D\\'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary\\'s Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine test reviews and test labels into test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_pos + test_neg\n",
    "test_label = test_label_p + test_label_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "replace_nospace = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\*)\")\n",
    "replace_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = [replace_nospace.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [replace_space.sub(\" \", line) for line in reviews]\n",
    "    #reviews = [reviews.lower() for line in reviews if (token.text not in punctuations)]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "train_data = preprocess_reviews(train_data)\n",
    "val_data = preprocess_reviews(val_data)\n",
    "test_data = preprocess_reviews(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bizarre horror movie filled with famous faces but stolen by cristina raines later of tvs flamingo road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the gateway to hell the scenes with raines modeling are very well captured the mood music is perfect deborah raffin is charming as cristinas pal but when raines moves into a creepy brooklyn heights brownstone inhabited by a blind priest on the top floor things really start cooking the neighbors including a fantastically wicked burgess meredith and kinky couple sylvia miles & beverly dangelo are a diabolical lot and eli wallach is great fun as a wily police detective the movie is nearly a cross pollination of rosemarys baby and the exorcist  but what a combination based on the best seller by jeffrey konvitz the sentinel is entertainingly spooky full of shocks brought off well by director michael winner who mounts a thoughtfully downbeat ending with skill 1 2 from '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# code from Lab 3\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1-gram token data from pickle\n",
    "import pickle as pkl\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "def tokenize_ngram_dataset(dataset, n):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        ngram = ngrams(sample.split(), n)\n",
    "        tokens = []\n",
    "        for gram in ngram:\n",
    "            tokens.append(gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "def merge_list(list1, list2):\n",
    "    mergedlist = []\n",
    "    for i in range(len(list1)):\n",
    "        element = list1[i] + list2[i]\n",
    "        mergedlist.append(element)\n",
    "    return mergedlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tokens_2, _ = tokenize_ngram_dataset(val_data, 2)\n",
    "val_data_tokens_bigram = merge_list(val_data_tokens, val_data_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens_2, all_train_tokens_2 = tokenize_ngram_dataset(train_data, 2)\n",
    "train_data_tokens_bigram = merge_list(train_data_tokens, train_data_tokens_2)\n",
    "all_train_tokens_bigram = all_train_tokens + all_train_tokens_2\n",
    "test_data_tokens_2, _ = tokenize_ngram_dataset(test_data, 2)\n",
    "test_data_tokens_bigram = merge_list(test_data_tokens, test_data_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tokens_3, _ = tokenize_ngram_dataset(val_data, 3)\n",
    "val_data_tokens_trigram = merge_list(val_data_tokens_bigram, val_data_tokens_3)\n",
    "train_data_tokens_3, all_train_tokens_3 = tokenize_ngram_dataset(train_data, 3)\n",
    "train_data_tokens_trigram = merge_list(train_data_tokens_bigram, train_data_tokens_3)\n",
    "all_train_tokens_trigram = all_train_tokens_bigram + all_train_tokens_3\n",
    "test_data_tokens_3, _ = tokenize_ngram_dataset(test_data, 3)\n",
    "test_data_tokens_trigram = merge_list(test_data_tokens_bigram, test_data_tokens_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tokens_4, _ = tokenize_ngram_dataset(val_data, 4)\n",
    "val_data_tokens_quagram = merge_list(val_data_tokens_trigram, val_data_tokens_4)\n",
    "train_data_tokens_4, all_train_tokens_4 = tokenize_ngram_dataset(train_data, 4)\n",
    "train_data_tokens_quagram = merge_list(train_data_tokens_trigram, train_data_tokens_4)\n",
    "all_train_tokens_quagram = all_train_tokens_trigram + all_train_tokens_4\n",
    "test_data_tokens_4, _ = tokenize_ngram_dataset(test_data, 4)\n",
    "test_data_tokens_quagram = merge_list(test_data_tokens_trigram, test_data_tokens_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_tokens_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bizarre',\n",
       " 'horror',\n",
       " 'movie',\n",
       " 'filled',\n",
       " 'with',\n",
       " 'famous',\n",
       " 'faces',\n",
       " 'but',\n",
       " 'stolen',\n",
       " 'by',\n",
       " 'cristina',\n",
       " 'raines',\n",
       " 'later',\n",
       " 'of',\n",
       " 'tvs',\n",
       " 'flamingo',\n",
       " 'road',\n",
       " 'as',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'but',\n",
       " 'somewhat',\n",
       " 'unstable',\n",
       " 'model',\n",
       " 'with',\n",
       " 'a',\n",
       " 'gummy',\n",
       " 'smile',\n",
       " 'who',\n",
       " 'is',\n",
       " 'slated',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'her',\n",
       " 'attempted',\n",
       " 'suicides',\n",
       " 'by',\n",
       " 'guarding',\n",
       " 'the',\n",
       " 'gateway',\n",
       " 'to',\n",
       " 'hell',\n",
       " 'the',\n",
       " 'scenes',\n",
       " 'with',\n",
       " 'raines',\n",
       " 'modeling',\n",
       " 'are',\n",
       " 'very',\n",
       " 'well',\n",
       " 'captured',\n",
       " 'the',\n",
       " 'mood',\n",
       " 'music',\n",
       " 'is',\n",
       " 'perfect',\n",
       " 'deborah',\n",
       " 'raffin',\n",
       " 'is',\n",
       " 'charming',\n",
       " 'as',\n",
       " 'cristinas',\n",
       " 'pal',\n",
       " 'but',\n",
       " 'when',\n",
       " 'raines',\n",
       " 'moves',\n",
       " 'into',\n",
       " 'a',\n",
       " 'creepy',\n",
       " 'brooklyn',\n",
       " 'heights',\n",
       " 'brownstone',\n",
       " 'inhabited',\n",
       " 'by',\n",
       " 'a',\n",
       " 'blind',\n",
       " 'priest',\n",
       " 'on',\n",
       " 'the',\n",
       " 'top',\n",
       " 'floor',\n",
       " 'things',\n",
       " 'really',\n",
       " 'start',\n",
       " 'cooking',\n",
       " 'the',\n",
       " 'neighbors',\n",
       " 'including',\n",
       " 'a',\n",
       " 'fantastically',\n",
       " 'wicked',\n",
       " 'burgess',\n",
       " 'meredith',\n",
       " 'and',\n",
       " 'kinky',\n",
       " 'couple',\n",
       " 'sylvia',\n",
       " 'miles',\n",
       " 'beverly',\n",
       " 'dangelo',\n",
       " 'are',\n",
       " 'a',\n",
       " 'diabolical',\n",
       " 'lot',\n",
       " 'and',\n",
       " 'eli',\n",
       " 'wallach',\n",
       " 'is',\n",
       " 'great',\n",
       " 'fun',\n",
       " 'as',\n",
       " 'a',\n",
       " 'wily',\n",
       " 'police',\n",
       " 'detective',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'nearly',\n",
       " 'a',\n",
       " 'cross',\n",
       " 'pollination',\n",
       " 'of',\n",
       " 'rosemarys',\n",
       " 'baby',\n",
       " 'and',\n",
       " 'the',\n",
       " 'exorcist',\n",
       " ' ',\n",
       " 'but',\n",
       " 'what',\n",
       " 'a',\n",
       " 'combination',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'best',\n",
       " 'seller',\n",
       " 'by',\n",
       " 'jeffrey',\n",
       " 'konvitz',\n",
       " 'the',\n",
       " 'sentinel',\n",
       " 'is',\n",
       " 'entertainingly',\n",
       " 'spooky',\n",
       " 'full',\n",
       " 'of',\n",
       " 'shocks',\n",
       " 'brought',\n",
       " 'off',\n",
       " 'well',\n",
       " 'by',\n",
       " 'director',\n",
       " 'michael',\n",
       " 'winner',\n",
       " 'who',\n",
       " 'mounts',\n",
       " 'a',\n",
       " 'thoughtfully',\n",
       " 'downbeat',\n",
       " 'ending',\n",
       " 'with',\n",
       " 'skill',\n",
       " '1',\n",
       " '2',\n",
       " 'from',\n",
       " ('bizarre', 'horror'),\n",
       " ('horror', 'movie'),\n",
       " ('movie', 'filled'),\n",
       " ('filled', 'with'),\n",
       " ('with', 'famous'),\n",
       " ('famous', 'faces'),\n",
       " ('faces', 'but'),\n",
       " ('but', 'stolen'),\n",
       " ('stolen', 'by'),\n",
       " ('by', 'cristina'),\n",
       " ('cristina', 'raines'),\n",
       " ('raines', 'later'),\n",
       " ('later', 'of'),\n",
       " ('of', 'tvs'),\n",
       " ('tvs', 'flamingo'),\n",
       " ('flamingo', 'road'),\n",
       " ('road', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'pretty'),\n",
       " ('pretty', 'but'),\n",
       " ('but', 'somewhat'),\n",
       " ('somewhat', 'unstable'),\n",
       " ('unstable', 'model'),\n",
       " ('model', 'with'),\n",
       " ('with', 'a'),\n",
       " ('a', 'gummy'),\n",
       " ('gummy', 'smile'),\n",
       " ('smile', 'who'),\n",
       " ('who', 'is'),\n",
       " ('is', 'slated'),\n",
       " ('slated', 'to'),\n",
       " ('to', 'pay'),\n",
       " ('pay', 'for'),\n",
       " ('for', 'her'),\n",
       " ('her', 'attempted'),\n",
       " ('attempted', 'suicides'),\n",
       " ('suicides', 'by'),\n",
       " ('by', 'guarding'),\n",
       " ('guarding', 'the'),\n",
       " ('the', 'gateway'),\n",
       " ('gateway', 'to'),\n",
       " ('to', 'hell'),\n",
       " ('hell', 'the'),\n",
       " ('the', 'scenes'),\n",
       " ('scenes', 'with'),\n",
       " ('with', 'raines'),\n",
       " ('raines', 'modeling'),\n",
       " ('modeling', 'are'),\n",
       " ('are', 'very'),\n",
       " ('very', 'well'),\n",
       " ('well', 'captured'),\n",
       " ('captured', 'the'),\n",
       " ('the', 'mood'),\n",
       " ('mood', 'music'),\n",
       " ('music', 'is'),\n",
       " ('is', 'perfect'),\n",
       " ('perfect', 'deborah'),\n",
       " ('deborah', 'raffin'),\n",
       " ('raffin', 'is'),\n",
       " ('is', 'charming'),\n",
       " ('charming', 'as'),\n",
       " ('as', 'cristinas'),\n",
       " ('cristinas', 'pal'),\n",
       " ('pal', 'but'),\n",
       " ('but', 'when'),\n",
       " ('when', 'raines'),\n",
       " ('raines', 'moves'),\n",
       " ('moves', 'into'),\n",
       " ('into', 'a'),\n",
       " ('a', 'creepy'),\n",
       " ('creepy', 'brooklyn'),\n",
       " ('brooklyn', 'heights'),\n",
       " ('heights', 'brownstone'),\n",
       " ('brownstone', 'inhabited'),\n",
       " ('inhabited', 'by'),\n",
       " ('by', 'a'),\n",
       " ('a', 'blind'),\n",
       " ('blind', 'priest'),\n",
       " ('priest', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'top'),\n",
       " ('top', 'floor'),\n",
       " ('floor', 'things'),\n",
       " ('things', 'really'),\n",
       " ('really', 'start'),\n",
       " ('start', 'cooking'),\n",
       " ('cooking', 'the'),\n",
       " ('the', 'neighbors'),\n",
       " ('neighbors', 'including'),\n",
       " ('including', 'a'),\n",
       " ('a', 'fantastically'),\n",
       " ('fantastically', 'wicked'),\n",
       " ('wicked', 'burgess'),\n",
       " ('burgess', 'meredith'),\n",
       " ('meredith', 'and'),\n",
       " ('and', 'kinky'),\n",
       " ('kinky', 'couple'),\n",
       " ('couple', 'sylvia'),\n",
       " ('sylvia', 'miles'),\n",
       " ('miles', '&'),\n",
       " ('&', 'beverly'),\n",
       " ('beverly', 'dangelo'),\n",
       " ('dangelo', 'are'),\n",
       " ('are', 'a'),\n",
       " ('a', 'diabolical'),\n",
       " ('diabolical', 'lot'),\n",
       " ('lot', 'and'),\n",
       " ('and', 'eli'),\n",
       " ('eli', 'wallach'),\n",
       " ('wallach', 'is'),\n",
       " ('is', 'great'),\n",
       " ('great', 'fun'),\n",
       " ('fun', 'as'),\n",
       " ('as', 'a'),\n",
       " ('a', 'wily'),\n",
       " ('wily', 'police'),\n",
       " ('police', 'detective'),\n",
       " ('detective', 'the'),\n",
       " ('the', 'movie'),\n",
       " ('movie', 'is'),\n",
       " ('is', 'nearly'),\n",
       " ('nearly', 'a'),\n",
       " ('a', 'cross'),\n",
       " ('cross', 'pollination'),\n",
       " ('pollination', 'of'),\n",
       " ('of', 'rosemarys'),\n",
       " ('rosemarys', 'baby'),\n",
       " ('baby', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'exorcist'),\n",
       " ('exorcist', 'but'),\n",
       " ('but', 'what'),\n",
       " ('what', 'a'),\n",
       " ('a', 'combination'),\n",
       " ('combination', 'based'),\n",
       " ('based', 'on'),\n",
       " ('on', 'the'),\n",
       " ('the', 'best'),\n",
       " ('best', 'seller'),\n",
       " ('seller', 'by'),\n",
       " ('by', 'jeffrey'),\n",
       " ('jeffrey', 'konvitz'),\n",
       " ('konvitz', 'the'),\n",
       " ('the', 'sentinel'),\n",
       " ('sentinel', 'is'),\n",
       " ('is', 'entertainingly'),\n",
       " ('entertainingly', 'spooky'),\n",
       " ('spooky', 'full'),\n",
       " ('full', 'of'),\n",
       " ('of', 'shocks'),\n",
       " ('shocks', 'brought'),\n",
       " ('brought', 'off'),\n",
       " ('off', 'well'),\n",
       " ('well', 'by'),\n",
       " ('by', 'director'),\n",
       " ('director', 'michael'),\n",
       " ('michael', 'winner'),\n",
       " ('winner', 'who'),\n",
       " ('who', 'mounts'),\n",
       " ('mounts', 'a'),\n",
       " ('a', 'thoughtfully'),\n",
       " ('thoughtfully', 'downbeat'),\n",
       " ('downbeat', 'ending'),\n",
       " ('ending', 'with'),\n",
       " ('with', 'skill'),\n",
       " ('skill', '1'),\n",
       " ('1', '2'),\n",
       " ('2', 'from'),\n",
       " ('bizarre', 'horror', 'movie'),\n",
       " ('horror', 'movie', 'filled'),\n",
       " ('movie', 'filled', 'with'),\n",
       " ('filled', 'with', 'famous'),\n",
       " ('with', 'famous', 'faces'),\n",
       " ('famous', 'faces', 'but'),\n",
       " ('faces', 'but', 'stolen'),\n",
       " ('but', 'stolen', 'by'),\n",
       " ('stolen', 'by', 'cristina'),\n",
       " ('by', 'cristina', 'raines'),\n",
       " ('cristina', 'raines', 'later'),\n",
       " ('raines', 'later', 'of'),\n",
       " ('later', 'of', 'tvs'),\n",
       " ('of', 'tvs', 'flamingo'),\n",
       " ('tvs', 'flamingo', 'road'),\n",
       " ('flamingo', 'road', 'as'),\n",
       " ('road', 'as', 'a'),\n",
       " ('as', 'a', 'pretty'),\n",
       " ('a', 'pretty', 'but'),\n",
       " ('pretty', 'but', 'somewhat'),\n",
       " ('but', 'somewhat', 'unstable'),\n",
       " ('somewhat', 'unstable', 'model'),\n",
       " ('unstable', 'model', 'with'),\n",
       " ('model', 'with', 'a'),\n",
       " ('with', 'a', 'gummy'),\n",
       " ('a', 'gummy', 'smile'),\n",
       " ('gummy', 'smile', 'who'),\n",
       " ('smile', 'who', 'is'),\n",
       " ('who', 'is', 'slated'),\n",
       " ('is', 'slated', 'to'),\n",
       " ('slated', 'to', 'pay'),\n",
       " ('to', 'pay', 'for'),\n",
       " ('pay', 'for', 'her'),\n",
       " ('for', 'her', 'attempted'),\n",
       " ('her', 'attempted', 'suicides'),\n",
       " ('attempted', 'suicides', 'by'),\n",
       " ('suicides', 'by', 'guarding'),\n",
       " ('by', 'guarding', 'the'),\n",
       " ('guarding', 'the', 'gateway'),\n",
       " ('the', 'gateway', 'to'),\n",
       " ('gateway', 'to', 'hell'),\n",
       " ('to', 'hell', 'the'),\n",
       " ('hell', 'the', 'scenes'),\n",
       " ('the', 'scenes', 'with'),\n",
       " ('scenes', 'with', 'raines'),\n",
       " ('with', 'raines', 'modeling'),\n",
       " ('raines', 'modeling', 'are'),\n",
       " ('modeling', 'are', 'very'),\n",
       " ('are', 'very', 'well'),\n",
       " ('very', 'well', 'captured'),\n",
       " ('well', 'captured', 'the'),\n",
       " ('captured', 'the', 'mood'),\n",
       " ('the', 'mood', 'music'),\n",
       " ('mood', 'music', 'is'),\n",
       " ('music', 'is', 'perfect'),\n",
       " ('is', 'perfect', 'deborah'),\n",
       " ('perfect', 'deborah', 'raffin'),\n",
       " ('deborah', 'raffin', 'is'),\n",
       " ('raffin', 'is', 'charming'),\n",
       " ('is', 'charming', 'as'),\n",
       " ('charming', 'as', 'cristinas'),\n",
       " ('as', 'cristinas', 'pal'),\n",
       " ('cristinas', 'pal', 'but'),\n",
       " ('pal', 'but', 'when'),\n",
       " ('but', 'when', 'raines'),\n",
       " ('when', 'raines', 'moves'),\n",
       " ('raines', 'moves', 'into'),\n",
       " ('moves', 'into', 'a'),\n",
       " ('into', 'a', 'creepy'),\n",
       " ('a', 'creepy', 'brooklyn'),\n",
       " ('creepy', 'brooklyn', 'heights'),\n",
       " ('brooklyn', 'heights', 'brownstone'),\n",
       " ('heights', 'brownstone', 'inhabited'),\n",
       " ('brownstone', 'inhabited', 'by'),\n",
       " ('inhabited', 'by', 'a'),\n",
       " ('by', 'a', 'blind'),\n",
       " ('a', 'blind', 'priest'),\n",
       " ('blind', 'priest', 'on'),\n",
       " ('priest', 'on', 'the'),\n",
       " ('on', 'the', 'top'),\n",
       " ('the', 'top', 'floor'),\n",
       " ('top', 'floor', 'things'),\n",
       " ('floor', 'things', 'really'),\n",
       " ('things', 'really', 'start'),\n",
       " ('really', 'start', 'cooking'),\n",
       " ('start', 'cooking', 'the'),\n",
       " ('cooking', 'the', 'neighbors'),\n",
       " ('the', 'neighbors', 'including'),\n",
       " ('neighbors', 'including', 'a'),\n",
       " ('including', 'a', 'fantastically'),\n",
       " ('a', 'fantastically', 'wicked'),\n",
       " ('fantastically', 'wicked', 'burgess'),\n",
       " ('wicked', 'burgess', 'meredith'),\n",
       " ('burgess', 'meredith', 'and'),\n",
       " ('meredith', 'and', 'kinky'),\n",
       " ('and', 'kinky', 'couple'),\n",
       " ('kinky', 'couple', 'sylvia'),\n",
       " ('couple', 'sylvia', 'miles'),\n",
       " ('sylvia', 'miles', '&'),\n",
       " ('miles', '&', 'beverly'),\n",
       " ('&', 'beverly', 'dangelo'),\n",
       " ('beverly', 'dangelo', 'are'),\n",
       " ('dangelo', 'are', 'a'),\n",
       " ('are', 'a', 'diabolical'),\n",
       " ('a', 'diabolical', 'lot'),\n",
       " ('diabolical', 'lot', 'and'),\n",
       " ('lot', 'and', 'eli'),\n",
       " ('and', 'eli', 'wallach'),\n",
       " ('eli', 'wallach', 'is'),\n",
       " ('wallach', 'is', 'great'),\n",
       " ('is', 'great', 'fun'),\n",
       " ('great', 'fun', 'as'),\n",
       " ('fun', 'as', 'a'),\n",
       " ('as', 'a', 'wily'),\n",
       " ('a', 'wily', 'police'),\n",
       " ('wily', 'police', 'detective'),\n",
       " ('police', 'detective', 'the'),\n",
       " ('detective', 'the', 'movie'),\n",
       " ('the', 'movie', 'is'),\n",
       " ('movie', 'is', 'nearly'),\n",
       " ('is', 'nearly', 'a'),\n",
       " ('nearly', 'a', 'cross'),\n",
       " ('a', 'cross', 'pollination'),\n",
       " ('cross', 'pollination', 'of'),\n",
       " ('pollination', 'of', 'rosemarys'),\n",
       " ('of', 'rosemarys', 'baby'),\n",
       " ('rosemarys', 'baby', 'and'),\n",
       " ('baby', 'and', 'the'),\n",
       " ('and', 'the', 'exorcist'),\n",
       " ('the', 'exorcist', 'but'),\n",
       " ('exorcist', 'but', 'what'),\n",
       " ('but', 'what', 'a'),\n",
       " ('what', 'a', 'combination'),\n",
       " ('a', 'combination', 'based'),\n",
       " ('combination', 'based', 'on'),\n",
       " ('based', 'on', 'the'),\n",
       " ('on', 'the', 'best'),\n",
       " ('the', 'best', 'seller'),\n",
       " ('best', 'seller', 'by'),\n",
       " ('seller', 'by', 'jeffrey'),\n",
       " ('by', 'jeffrey', 'konvitz'),\n",
       " ('jeffrey', 'konvitz', 'the'),\n",
       " ('konvitz', 'the', 'sentinel'),\n",
       " ('the', 'sentinel', 'is'),\n",
       " ('sentinel', 'is', 'entertainingly'),\n",
       " ('is', 'entertainingly', 'spooky'),\n",
       " ('entertainingly', 'spooky', 'full'),\n",
       " ('spooky', 'full', 'of'),\n",
       " ('full', 'of', 'shocks'),\n",
       " ('of', 'shocks', 'brought'),\n",
       " ('shocks', 'brought', 'off'),\n",
       " ('brought', 'off', 'well'),\n",
       " ('off', 'well', 'by'),\n",
       " ('well', 'by', 'director'),\n",
       " ('by', 'director', 'michael'),\n",
       " ('director', 'michael', 'winner'),\n",
       " ('michael', 'winner', 'who'),\n",
       " ('winner', 'who', 'mounts'),\n",
       " ('who', 'mounts', 'a'),\n",
       " ('mounts', 'a', 'thoughtfully'),\n",
       " ('a', 'thoughtfully', 'downbeat'),\n",
       " ('thoughtfully', 'downbeat', 'ending'),\n",
       " ('downbeat', 'ending', 'with'),\n",
       " ('ending', 'with', 'skill'),\n",
       " ('with', 'skill', '1'),\n",
       " ('skill', '1', '2'),\n",
       " ('1', '2', 'from'),\n",
       " ('bizarre', 'horror', 'movie', 'filled'),\n",
       " ('horror', 'movie', 'filled', 'with'),\n",
       " ('movie', 'filled', 'with', 'famous'),\n",
       " ('filled', 'with', 'famous', 'faces'),\n",
       " ('with', 'famous', 'faces', 'but'),\n",
       " ('famous', 'faces', 'but', 'stolen'),\n",
       " ('faces', 'but', 'stolen', 'by'),\n",
       " ('but', 'stolen', 'by', 'cristina'),\n",
       " ('stolen', 'by', 'cristina', 'raines'),\n",
       " ('by', 'cristina', 'raines', 'later'),\n",
       " ('cristina', 'raines', 'later', 'of'),\n",
       " ('raines', 'later', 'of', 'tvs'),\n",
       " ('later', 'of', 'tvs', 'flamingo'),\n",
       " ('of', 'tvs', 'flamingo', 'road'),\n",
       " ('tvs', 'flamingo', 'road', 'as'),\n",
       " ('flamingo', 'road', 'as', 'a'),\n",
       " ('road', 'as', 'a', 'pretty'),\n",
       " ('as', 'a', 'pretty', 'but'),\n",
       " ('a', 'pretty', 'but', 'somewhat'),\n",
       " ('pretty', 'but', 'somewhat', 'unstable'),\n",
       " ('but', 'somewhat', 'unstable', 'model'),\n",
       " ('somewhat', 'unstable', 'model', 'with'),\n",
       " ('unstable', 'model', 'with', 'a'),\n",
       " ('model', 'with', 'a', 'gummy'),\n",
       " ('with', 'a', 'gummy', 'smile'),\n",
       " ('a', 'gummy', 'smile', 'who'),\n",
       " ('gummy', 'smile', 'who', 'is'),\n",
       " ('smile', 'who', 'is', 'slated'),\n",
       " ('who', 'is', 'slated', 'to'),\n",
       " ('is', 'slated', 'to', 'pay'),\n",
       " ('slated', 'to', 'pay', 'for'),\n",
       " ('to', 'pay', 'for', 'her'),\n",
       " ('pay', 'for', 'her', 'attempted'),\n",
       " ('for', 'her', 'attempted', 'suicides'),\n",
       " ('her', 'attempted', 'suicides', 'by'),\n",
       " ('attempted', 'suicides', 'by', 'guarding'),\n",
       " ('suicides', 'by', 'guarding', 'the'),\n",
       " ('by', 'guarding', 'the', 'gateway'),\n",
       " ('guarding', 'the', 'gateway', 'to'),\n",
       " ('the', 'gateway', 'to', 'hell'),\n",
       " ('gateway', 'to', 'hell', 'the'),\n",
       " ('to', 'hell', 'the', 'scenes'),\n",
       " ('hell', 'the', 'scenes', 'with'),\n",
       " ('the', 'scenes', 'with', 'raines'),\n",
       " ('scenes', 'with', 'raines', 'modeling'),\n",
       " ('with', 'raines', 'modeling', 'are'),\n",
       " ('raines', 'modeling', 'are', 'very'),\n",
       " ('modeling', 'are', 'very', 'well'),\n",
       " ('are', 'very', 'well', 'captured'),\n",
       " ('very', 'well', 'captured', 'the'),\n",
       " ('well', 'captured', 'the', 'mood'),\n",
       " ('captured', 'the', 'mood', 'music'),\n",
       " ('the', 'mood', 'music', 'is'),\n",
       " ('mood', 'music', 'is', 'perfect'),\n",
       " ('music', 'is', 'perfect', 'deborah'),\n",
       " ('is', 'perfect', 'deborah', 'raffin'),\n",
       " ('perfect', 'deborah', 'raffin', 'is'),\n",
       " ('deborah', 'raffin', 'is', 'charming'),\n",
       " ('raffin', 'is', 'charming', 'as'),\n",
       " ('is', 'charming', 'as', 'cristinas'),\n",
       " ('charming', 'as', 'cristinas', 'pal'),\n",
       " ('as', 'cristinas', 'pal', 'but'),\n",
       " ('cristinas', 'pal', 'but', 'when'),\n",
       " ('pal', 'but', 'when', 'raines'),\n",
       " ('but', 'when', 'raines', 'moves'),\n",
       " ('when', 'raines', 'moves', 'into'),\n",
       " ('raines', 'moves', 'into', 'a'),\n",
       " ('moves', 'into', 'a', 'creepy'),\n",
       " ('into', 'a', 'creepy', 'brooklyn'),\n",
       " ('a', 'creepy', 'brooklyn', 'heights'),\n",
       " ('creepy', 'brooklyn', 'heights', 'brownstone'),\n",
       " ('brooklyn', 'heights', 'brownstone', 'inhabited'),\n",
       " ('heights', 'brownstone', 'inhabited', 'by'),\n",
       " ('brownstone', 'inhabited', 'by', 'a'),\n",
       " ('inhabited', 'by', 'a', 'blind'),\n",
       " ('by', 'a', 'blind', 'priest'),\n",
       " ('a', 'blind', 'priest', 'on'),\n",
       " ('blind', 'priest', 'on', 'the'),\n",
       " ('priest', 'on', 'the', 'top'),\n",
       " ('on', 'the', 'top', 'floor'),\n",
       " ('the', 'top', 'floor', 'things'),\n",
       " ('top', 'floor', 'things', 'really'),\n",
       " ('floor', 'things', 'really', 'start'),\n",
       " ('things', 'really', 'start', 'cooking'),\n",
       " ('really', 'start', 'cooking', 'the'),\n",
       " ('start', 'cooking', 'the', 'neighbors'),\n",
       " ('cooking', 'the', 'neighbors', 'including'),\n",
       " ('the', 'neighbors', 'including', 'a'),\n",
       " ('neighbors', 'including', 'a', 'fantastically'),\n",
       " ('including', 'a', 'fantastically', 'wicked'),\n",
       " ('a', 'fantastically', 'wicked', 'burgess'),\n",
       " ('fantastically', 'wicked', 'burgess', 'meredith'),\n",
       " ('wicked', 'burgess', 'meredith', 'and'),\n",
       " ('burgess', 'meredith', 'and', 'kinky'),\n",
       " ('meredith', 'and', 'kinky', 'couple'),\n",
       " ('and', 'kinky', 'couple', 'sylvia'),\n",
       " ('kinky', 'couple', 'sylvia', 'miles'),\n",
       " ('couple', 'sylvia', 'miles', '&'),\n",
       " ('sylvia', 'miles', '&', 'beverly'),\n",
       " ('miles', '&', 'beverly', 'dangelo'),\n",
       " ('&', 'beverly', 'dangelo', 'are'),\n",
       " ('beverly', 'dangelo', 'are', 'a'),\n",
       " ('dangelo', 'are', 'a', 'diabolical'),\n",
       " ('are', 'a', 'diabolical', 'lot'),\n",
       " ('a', 'diabolical', 'lot', 'and'),\n",
       " ('diabolical', 'lot', 'and', 'eli'),\n",
       " ('lot', 'and', 'eli', 'wallach'),\n",
       " ('and', 'eli', 'wallach', 'is'),\n",
       " ('eli', 'wallach', 'is', 'great'),\n",
       " ('wallach', 'is', 'great', 'fun'),\n",
       " ('is', 'great', 'fun', 'as'),\n",
       " ('great', 'fun', 'as', 'a'),\n",
       " ('fun', 'as', 'a', 'wily'),\n",
       " ('as', 'a', 'wily', 'police'),\n",
       " ('a', 'wily', 'police', 'detective'),\n",
       " ('wily', 'police', 'detective', 'the'),\n",
       " ('police', 'detective', 'the', 'movie'),\n",
       " ('detective', 'the', 'movie', 'is'),\n",
       " ('the', 'movie', 'is', 'nearly'),\n",
       " ('movie', 'is', 'nearly', 'a'),\n",
       " ('is', 'nearly', 'a', 'cross'),\n",
       " ('nearly', 'a', 'cross', 'pollination'),\n",
       " ('a', 'cross', 'pollination', 'of'),\n",
       " ('cross', 'pollination', 'of', 'rosemarys'),\n",
       " ('pollination', 'of', 'rosemarys', 'baby'),\n",
       " ('of', 'rosemarys', 'baby', 'and'),\n",
       " ('rosemarys', 'baby', 'and', 'the'),\n",
       " ('baby', 'and', 'the', 'exorcist'),\n",
       " ('and', 'the', 'exorcist', 'but'),\n",
       " ('the', 'exorcist', 'but', 'what'),\n",
       " ('exorcist', 'but', 'what', 'a'),\n",
       " ('but', 'what', 'a', 'combination'),\n",
       " ('what', 'a', 'combination', 'based'),\n",
       " ('a', 'combination', 'based', 'on'),\n",
       " ('combination', 'based', 'on', 'the'),\n",
       " ('based', 'on', 'the', 'best'),\n",
       " ('on', 'the', 'best', 'seller'),\n",
       " ('the', 'best', 'seller', 'by'),\n",
       " ('best', 'seller', 'by', 'jeffrey'),\n",
       " ('seller', 'by', 'jeffrey', 'konvitz'),\n",
       " ('by', 'jeffrey', 'konvitz', 'the'),\n",
       " ('jeffrey', 'konvitz', 'the', 'sentinel'),\n",
       " ('konvitz', 'the', 'sentinel', 'is'),\n",
       " ('the', 'sentinel', 'is', 'entertainingly'),\n",
       " ('sentinel', 'is', 'entertainingly', 'spooky'),\n",
       " ('is', 'entertainingly', 'spooky', 'full'),\n",
       " ('entertainingly', 'spooky', 'full', 'of'),\n",
       " ('spooky', 'full', 'of', 'shocks'),\n",
       " ('full', 'of', 'shocks', 'brought'),\n",
       " ('of', 'shocks', 'brought', 'off'),\n",
       " ('shocks', 'brought', 'off', 'well'),\n",
       " ('brought', 'off', 'well', 'by'),\n",
       " ('off', 'well', 'by', 'director'),\n",
       " ('well', 'by', 'director', 'michael'),\n",
       " ('by', 'director', 'michael', 'winner'),\n",
       " ('director', 'michael', 'winner', 'who'),\n",
       " ('michael', 'winner', 'who', 'mounts'),\n",
       " ('winner', 'who', 'mounts', 'a'),\n",
       " ('who', 'mounts', 'a', 'thoughtfully'),\n",
       " ('mounts', 'a', 'thoughtfully', 'downbeat'),\n",
       " ('a', 'thoughtfully', 'downbeat', 'ending'),\n",
       " ('thoughtfully', 'downbeat', 'ending', 'with'),\n",
       " ('downbeat', 'ending', 'with', 'skill'),\n",
       " ('ending', 'with', 'skill', '1'),\n",
       " ('with', 'skill', '1', '2'),\n",
       " ('skill', '1', '2', 'from')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokens_quagram[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens_quagram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 2,\n",
       " 'and': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'to': 6,\n",
       " 'is': 7,\n",
       " 'in': 8,\n",
       " 'i': 9,\n",
       " 'it': 10,\n",
       " 'this': 11,\n",
       " 'that': 12,\n",
       " 'was': 13,\n",
       " 'as': 14,\n",
       " 'for': 15,\n",
       " 'with': 16,\n",
       " 'movie': 17,\n",
       " 'but': 18,\n",
       " ('of', 'the'): 19,\n",
       " 'film': 20,\n",
       " 'you': 21,\n",
       " 'on': 22,\n",
       " 'nt': 23,\n",
       " 'not': 24,\n",
       " 'are': 25,\n",
       " 'he': 26,\n",
       " 'his': 27,\n",
       " 'have': 28,\n",
       " 'be': 29,\n",
       " 'one': 30,\n",
       " ('in', 'the'): 31,\n",
       " 'its': 32,\n",
       " 'all': 33,\n",
       " 'at': 34,\n",
       " 'they': 35,\n",
       " 'by': 36,\n",
       " 'an': 37,\n",
       " 'who': 38,\n",
       " 'so': 39,\n",
       " 'from': 40,\n",
       " 'like': 41,\n",
       " 'there': 42,\n",
       " 'her': 43,\n",
       " 'or': 44,\n",
       " 'just': 45,\n",
       " 'do': 46,\n",
       " 'about': 47,\n",
       " 'has': 48,\n",
       " 'out': 49,\n",
       " 'if': 50,\n",
       " 'what': 51,\n",
       " 'some': 52,\n",
       " ('this', 'movie'): 53,\n",
       " 'good': 54,\n",
       " ' ': 55,\n",
       " 'when': 56,\n",
       " 'more': 57,\n",
       " 'very': 58,\n",
       " 'she': 59,\n",
       " 'up': 60,\n",
       " ('and', 'the'): 61,\n",
       " 'would': 62,\n",
       " 's': 63,\n",
       " ('is', 'a'): 64,\n",
       " 'no': 65,\n",
       " 'even': 66,\n",
       " ('the', 'film'): 67,\n",
       " 'time': 68,\n",
       " 'my': 69,\n",
       " 'can': 70,\n",
       " ('to', 'the'): 71,\n",
       " 'only': 72,\n",
       " 'which': 73,\n",
       " ('to', 'be'): 74,\n",
       " 'were': 75,\n",
       " 'story': 76,\n",
       " 'really': 77,\n",
       " 'their': 78,\n",
       " 'had': 79,\n",
       " 'see': 80,\n",
       " ('the', 'movie'): 81,\n",
       " ('this', 'film'): 82,\n",
       " 'me': 83,\n",
       " 'did': 84,\n",
       " 'well': 85,\n",
       " 'does': 86,\n",
       " '  ': 87,\n",
       " 'we': 88,\n",
       " ('it', 'is'): 89,\n",
       " 'than': 90,\n",
       " 'much': 91,\n",
       " ('this', 'is'): 92,\n",
       " 'could': 93,\n",
       " 'get': 94,\n",
       " 'been': 95,\n",
       " 'bad': 96,\n",
       " 'into': 97,\n",
       " 'will': 98,\n",
       " 'also': 99,\n",
       " 'people': 100,\n",
       " 'other': 101,\n",
       " 'first': 102,\n",
       " 'him': 103,\n",
       " 'great': 104,\n",
       " ('on', 'the'): 105,\n",
       " 'because': 106,\n",
       " ('in', 'a'): 107,\n",
       " 'how': 108,\n",
       " 'most': 109,\n",
       " 'made': 110,\n",
       " ('it', 'was'): 111,\n",
       " ('one', 'of'): 112,\n",
       " 'then': 113,\n",
       " 'make': 114,\n",
       " 'movies': 115,\n",
       " 'way': 116,\n",
       " ('for', 'the'): 117,\n",
       " 'them': 118,\n",
       " 'films': 119,\n",
       " ('with', 'the'): 120,\n",
       " ('of', 'a'): 121,\n",
       " 'too': 122,\n",
       " 'any': 123,\n",
       " 'after': 124,\n",
       " 'characters': 125,\n",
       " 'think': 126,\n",
       " ('is', 'the'): 127,\n",
       " 'two': 128,\n",
       " ('as', 'a'): 129,\n",
       " ('at', 'the'): 130,\n",
       " 'watch': 131,\n",
       " 'many': 132,\n",
       " 'life': 133,\n",
       " 'seen': 134,\n",
       " 'being': 135,\n",
       " 'character': 136,\n",
       " 'never': 137,\n",
       " 'plot': 138,\n",
       " 'best': 139,\n",
       " 'acting': 140,\n",
       " 'love': 141,\n",
       " 'where': 142,\n",
       " 'little': 143,\n",
       " 'over': 144,\n",
       " ('from', 'the'): 145,\n",
       " 'show': 146,\n",
       " 'know': 147,\n",
       " ('in', 'this'): 148,\n",
       " ('with', 'a'): 149,\n",
       " 'off': 150,\n",
       " 'ever': 151,\n",
       " ('as', 'the'): 152,\n",
       " 'better': 153,\n",
       " 'your': 154,\n",
       " 'still': 155,\n",
       " ('if', 'you'): 156,\n",
       " 'end': 157,\n",
       " 'man': 158,\n",
       " 'these': 159,\n",
       " 'here': 160,\n",
       " 'should': 161,\n",
       " 'scene': 162,\n",
       " 'while': 163,\n",
       " 'why': 164,\n",
       " 'say': 165,\n",
       " 'scenes': 166,\n",
       " ('i', 'was'): 167,\n",
       " ('the', 'story'): 168,\n",
       " ('to', 'see'): 169,\n",
       " 'such': 170,\n",
       " ('out', 'of'): 171,\n",
       " ('that', 'the'): 172,\n",
       " 've': 173,\n",
       " 'go': 174,\n",
       " 'something': 175,\n",
       " 'back': 176,\n",
       " ('one', 'of', 'the'): 177,\n",
       " 'through': 178,\n",
       " 'm': 179,\n",
       " ('by', 'the'): 180,\n",
       " 'those': 181,\n",
       " 'real': 182,\n",
       " ('movie', 'is'): 183,\n",
       " ('was', 'a'): 184,\n",
       " 'years': 185,\n",
       " ('the', 'first'): 186,\n",
       " 'watching': 187,\n",
       " 'old': 188,\n",
       " ('for', 'a'): 189,\n",
       " 'now': 190,\n",
       " ('and', 'i'): 191,\n",
       " ('i', 'have'): 192,\n",
       " 'though': 193,\n",
       " 'actors': 194,\n",
       " ('there', 'is'): 195,\n",
       " 'thing': 196,\n",
       " ('all', 'the'): 197,\n",
       " 'work': 198,\n",
       " 'new': 199,\n",
       " 'another': 200,\n",
       " ('have', 'been'): 201,\n",
       " 'before': 202,\n",
       " '10': 203,\n",
       " ('of', 'this'): 204,\n",
       " 'funny': 205,\n",
       " 'nothing': 206,\n",
       " 'makes': 207,\n",
       " 'actually': 208,\n",
       " 'director': 209,\n",
       " 'find': 210,\n",
       " ('film', 'is'): 211,\n",
       " 'look': 212,\n",
       " ('and', 'a'): 213,\n",
       " 'going': 214,\n",
       " 'few': 215,\n",
       " ('is', 'not'): 216,\n",
       " 'part': 217,\n",
       " 'us': 218,\n",
       " 'same': 219,\n",
       " 'every': 220,\n",
       " 'lot': 221,\n",
       " 'again': 222,\n",
       " 're': 223,\n",
       " ('the', 'end'): 224,\n",
       " 'cast': 225,\n",
       " 'quite': 226,\n",
       " 'things': 227,\n",
       " 'ca': 228,\n",
       " 'world': 229,\n",
       " 'got': 230,\n",
       " ('a', 'good'): 231,\n",
       " 'want': 232,\n",
       " ('there', 'are'): 233,\n",
       " 'horror': 234,\n",
       " 'young': 235,\n",
       " 'down': 236,\n",
       " 'pretty': 237,\n",
       " 'around': 238,\n",
       " ('a', 'lot'): 239,\n",
       " ('the', 'same'): 240,\n",
       " 'seems': 241,\n",
       " 'fact': 242,\n",
       " ('the', 'most'): 243,\n",
       " 'big': 244,\n",
       " ('the', 'best'): 245,\n",
       " 'take': 246,\n",
       " 'however': 247,\n",
       " 'enough': 248,\n",
       " 'series': 249,\n",
       " ('but', 'the'): 250,\n",
       " 'between': 251,\n",
       " 'thought': 252,\n",
       " 'long': 253,\n",
       " 'both': 254,\n",
       " 'original': 255,\n",
       " 'may': 256,\n",
       " 'give': 257,\n",
       " 'own': 258,\n",
       " ('to', 'make'): 259,\n",
       " 'action': 260,\n",
       " ('about', 'the'): 261,\n",
       " 'right': 262,\n",
       " 'always': 263,\n",
       " 'without': 264,\n",
       " 'gets': 265,\n",
       " 'point': 266,\n",
       " 'must': 267,\n",
       " ('of', 'his'): 268,\n",
       " ('that', 'is'): 269,\n",
       " 'times': 270,\n",
       " 'come': 271,\n",
       " ('the', 'only'): 272,\n",
       " 'comedy': 273,\n",
       " ('i', 'think'): 274,\n",
       " ('on', 'a'): 275,\n",
       " 'family': 276,\n",
       " ('a', 'movie'): 277,\n",
       " 'saw': 278,\n",
       " 'almost': 279,\n",
       " ('he', 'is'): 280,\n",
       " ('but', 'i'): 281,\n",
       " 'role': 282,\n",
       " 'interesting': 283,\n",
       " 'least': 284,\n",
       " ('that', 'i'): 285,\n",
       " 'whole': 286,\n",
       " 'bit': 287,\n",
       " 'done': 288,\n",
       " ('to', 'watch'): 289,\n",
       " ('the', 'plot'): 290,\n",
       " ('this', 'one'): 291,\n",
       " 'music': 292,\n",
       " 'guy': 293,\n",
       " 'far': 294,\n",
       " 'making': 295,\n",
       " 'script': 296,\n",
       " 'might': 297,\n",
       " ('a', 'great'): 298,\n",
       " 'minutes': 299,\n",
       " 'feel': 300,\n",
       " ('a', 'very'): 301,\n",
       " 'last': 302,\n",
       " 'since': 303,\n",
       " ('the', 'characters'): 304,\n",
       " 'anything': 305,\n",
       " ('to', 'a'): 306,\n",
       " ('that', 'it'): 307,\n",
       " ('but', 'it'): 308,\n",
       " 'tv': 309,\n",
       " 'performance': 310,\n",
       " 'probably': 311,\n",
       " ('a', 'few'): 312,\n",
       " ('be', 'a'): 313,\n",
       " 'kind': 314,\n",
       " 'am': 315,\n",
       " '2': 316,\n",
       " ('and', 'it'): 317,\n",
       " 'worst': 318,\n",
       " 'away': 319,\n",
       " 'rather': 320,\n",
       " ('have', 'to'): 321,\n",
       " ('some', 'of'): 322,\n",
       " 'day': 323,\n",
       " 'girl': 324,\n",
       " ('this', 'movie', 'is'): 325,\n",
       " 'yet': 326,\n",
       " 'fun': 327,\n",
       " ('to', 'get'): 328,\n",
       " 'woman': 329,\n",
       " ('its', 'a'): 330,\n",
       " ('was', 'the'): 331,\n",
       " 'sure': 332,\n",
       " 'hard': 333,\n",
       " ('i', 'dont'): 334,\n",
       " ('they', 'are'): 335,\n",
       " 'having': 336,\n",
       " ('want', 'to'): 337,\n",
       " ('is', 'that'): 338,\n",
       " 'found': 339,\n",
       " 'each': 340,\n",
       " 'our': 341,\n",
       " ('a', 'little'): 342,\n",
       " 'anyone': 343,\n",
       " 'comes': 344,\n",
       " 'd': 345,\n",
       " ('into', 'the'): 346,\n",
       " 'played': 347,\n",
       " 'although': 348,\n",
       " 'believe': 349,\n",
       " ('the', 'other'): 350,\n",
       " 'looking': 351,\n",
       " ('to', 'do'): 352,\n",
       " ('would', 'have'): 353,\n",
       " ('the', 'acting'): 354,\n",
       " 'trying': 355,\n",
       " 'goes': 356,\n",
       " 'especially': 357,\n",
       " ('has', 'a'): 358,\n",
       " 'course': 359,\n",
       " ('at', 'least'): 360,\n",
       " 'screen': 361,\n",
       " ('the', 'way'): 362,\n",
       " ('of', 'the', 'film'): 363,\n",
       " 'set': 364,\n",
       " 'place': 365,\n",
       " ('a', 'film'): 366,\n",
       " ('like', 'a'): 367,\n",
       " ('i', 'am'): 368,\n",
       " ('lot', 'of'): 369,\n",
       " 'different': 370,\n",
       " 'shows': 371,\n",
       " 'book': 372,\n",
       " ('this', 'is', 'a'): 373,\n",
       " 'looks': 374,\n",
       " 'put': 375,\n",
       " 'three': 376,\n",
       " 'money': 377,\n",
       " ('as', 'well'): 378,\n",
       " ('is', 'one'): 379,\n",
       " 'ending': 380,\n",
       " 'everything': 381,\n",
       " ('kind', 'of'): 382,\n",
       " 'once': 383,\n",
       " 'actor': 384,\n",
       " 'true': 385,\n",
       " 'sense': 386,\n",
       " 'reason': 387,\n",
       " ('would', 'be'): 388,\n",
       " 'year': 389,\n",
       " 'maybe': 390,\n",
       " ('i', 'would'): 391,\n",
       " 'job': 392,\n",
       " 'worth': 393,\n",
       " ('a', 'lot', 'of'): 394,\n",
       " 'dvd': 395,\n",
       " ('this', 'was'): 396,\n",
       " 'main': 397,\n",
       " 'together': 398,\n",
       " ('to', 'have'): 399,\n",
       " 'plays': 400,\n",
       " ('trying', 'to'): 401,\n",
       " ('have', 'a'): 402,\n",
       " 'someone': 403,\n",
       " 'play': 404,\n",
       " ('the', 'whole'): 405,\n",
       " 'watched': 406,\n",
       " 'american': 407,\n",
       " 'said': 408,\n",
       " 'later': 409,\n",
       " 'effects': 410,\n",
       " 'house': 411,\n",
       " ('he', 'was'): 412,\n",
       " 'takes': 413,\n",
       " 'audience': 414,\n",
       " 'high': 415,\n",
       " 'beautiful': 416,\n",
       " 'himself': 417,\n",
       " ('like', 'the'): 418,\n",
       " 'john': 419,\n",
       " 'night': 420,\n",
       " ('of', 'course'): 421,\n",
       " 'everyone': 422,\n",
       " 'seem': 423,\n",
       " ('that', 'this'): 424,\n",
       " ('and', 'his'): 425,\n",
       " 'instead': 426,\n",
       " ('of', 'the', 'movie'): 427,\n",
       " 'during': 428,\n",
       " ('a', 'bit'): 429,\n",
       " ('you', 'can'): 430,\n",
       " 'special': 431,\n",
       " 'left': 432,\n",
       " ('the', 'worst'): 433,\n",
       " ('in', 'his'): 434,\n",
       " ('most', 'of'): 435,\n",
       " ('the', 'original'): 436,\n",
       " 'version': 437,\n",
       " 'wife': 438,\n",
       " 'half': 439,\n",
       " 'star': 440,\n",
       " 'war': 441,\n",
       " 'idea': 442,\n",
       " 'seeing': 443,\n",
       " ('that', 'he'): 444,\n",
       " ('movie', 'was'): 445,\n",
       " 'shot': 446,\n",
       " 'excellent': 447,\n",
       " ('the', 'time'): 448,\n",
       " ('at', 'all'): 449,\n",
       " 'black': 450,\n",
       " ('not', 'a'): 451,\n",
       " 'nice': 452,\n",
       " 'less': 453,\n",
       " ('is', 'an'): 454,\n",
       " ('which', 'is'): 455,\n",
       " 'mind': 456,\n",
       " ('more', 'than'): 457,\n",
       " '   ': 458,\n",
       " 'second': 459,\n",
       " ('going', 'to'): 460,\n",
       " ('could', 'have'): 461,\n",
       " ('i', 'can'): 462,\n",
       " ('to', 'say'): 463,\n",
       " 'read': 464,\n",
       " 'father': 465,\n",
       " 'home': 466,\n",
       " 'simply': 467,\n",
       " 'men': 468,\n",
       " '1': 469,\n",
       " 'else': 470,\n",
       " 'death': 471,\n",
       " 'completely': 472,\n",
       " 'kids': 473,\n",
       " 'help': 474,\n",
       " 'poor': 475,\n",
       " 'dead': 476,\n",
       " ('i', 'had'): 477,\n",
       " ('who', 'is'): 478,\n",
       " 'fan': 479,\n",
       " 'line': 480,\n",
       " ('the', 'film', 'is'): 481,\n",
       " 'used': 482,\n",
       " 'friends': 483,\n",
       " 'classic': 484,\n",
       " ('by', 'a'): 485,\n",
       " 'either': 486,\n",
       " ('some', 'of', 'the'): 487,\n",
       " 'budget': 488,\n",
       " 'given': 489,\n",
       " 'top': 490,\n",
       " 'low': 491,\n",
       " 'short': 492,\n",
       " 'use': 493,\n",
       " 'need': 494,\n",
       " ('all', 'of'): 495,\n",
       " ('i', 'saw'): 496,\n",
       " 'performances': 497,\n",
       " 'hollywood': 498,\n",
       " ('but', 'this'): 499,\n",
       " ('such', 'a'): 500,\n",
       " ('of', 'them'): 501,\n",
       " '3': 502,\n",
       " 'enjoy': 503,\n",
       " 'try': 504,\n",
       " ('this', 'film', 'is'): 505,\n",
       " 'full': 506,\n",
       " 'until': 507,\n",
       " 'next': 508,\n",
       " ('and', 'then'): 509,\n",
       " ('when', 'i'): 510,\n",
       " 'production': 511,\n",
       " ('is', 'one', 'of'): 512,\n",
       " 'women': 513,\n",
       " ('is', 'so'): 514,\n",
       " 'camera': 515,\n",
       " ('into', 'a'): 516,\n",
       " 'rest': 517,\n",
       " 'along': 518,\n",
       " 'boring': 519,\n",
       " 'wrong': 520,\n",
       " 'truly': 521,\n",
       " ('of', 'all'): 522,\n",
       " 'awful': 523,\n",
       " 'video': 524,\n",
       " ('seems', 'to'): 525,\n",
       " ('movie', 'and'): 526,\n",
       " ('when', 'the'): 527,\n",
       " 'sex': 528,\n",
       " ('of', 'it'): 529,\n",
       " ('over', 'the'): 530,\n",
       " ('the', 'fact'): 531,\n",
       " 'tell': 532,\n",
       " ('part', 'of'): 533,\n",
       " 'start': 534,\n",
       " 'remember': 535,\n",
       " ('its', 'not'): 536,\n",
       " 'stars': 537,\n",
       " ('in', 'my'): 538,\n",
       " ('movie', 'i'): 539,\n",
       " 'small': 540,\n",
       " 'couple': 541,\n",
       " 'came': 542,\n",
       " 'episode': 543,\n",
       " ('it', 'has'): 544,\n",
       " 'face': 545,\n",
       " 'mean': 546,\n",
       " 'perhaps': 547,\n",
       " ('like', 'this'): 548,\n",
       " ('she', 'is'): 549,\n",
       " 'moments': 550,\n",
       " ('is', 'just'): 551,\n",
       " 'often': 552,\n",
       " 'recommend': 553,\n",
       " 'others': 554,\n",
       " 'let': 555,\n",
       " 'school': 556,\n",
       " 'keep': 557,\n",
       " 'wonderful': 558,\n",
       " 'll': 559,\n",
       " 'stupid': 560,\n",
       " ('i', 'thought'): 561,\n",
       " 'playing': 562,\n",
       " ('the', 'world'): 563,\n",
       " 'getting': 564,\n",
       " 'early': 565,\n",
       " 'understand': 566,\n",
       " 'name': 567,\n",
       " ('the', 'rest'): 568,\n",
       " ('with', 'his'): 569,\n",
       " ('the', 'main'): 570,\n",
       " 'gives': 571,\n",
       " ('fact', 'that'): 572,\n",
       " 'written': 573,\n",
       " 'doing': 574,\n",
       " 'terrible': 575,\n",
       " 'perfect': 576,\n",
       " ('to', 'be', 'a'): 577,\n",
       " 'human': 578,\n",
       " ('as', 'it'): 579,\n",
       " 'lost': 580,\n",
       " ('for', 'this'): 581,\n",
       " ('you', 'have'): 582,\n",
       " 'itself': 583,\n",
       " 'piece': 584,\n",
       " ('from', 'a'): 585,\n",
       " 'become': 586,\n",
       " 'lines': 587,\n",
       " ('the', 'two'): 588,\n",
       " ('they', 'were'): 589,\n",
       " 'person': 590,\n",
       " 'definitely': 591,\n",
       " 'supposed': 592,\n",
       " 'felt': 593,\n",
       " 'case': 594,\n",
       " ('film', 'and'): 595,\n",
       " 'live': 596,\n",
       " 'dialogue': 597,\n",
       " ('has', 'been'): 598,\n",
       " 'finally': 599,\n",
       " ('is', 'very'): 600,\n",
       " 'style': 601,\n",
       " ('the', 'actors'): 602,\n",
       " ('you', 'are'): 603,\n",
       " ('the', 'movie', 'is'): 604,\n",
       " 'head': 605,\n",
       " ('through', 'the'): 606,\n",
       " 'liked': 607,\n",
       " ('that', 'was'): 608,\n",
       " ('that', 'they'): 609,\n",
       " 'sort': 610,\n",
       " 'title': 611,\n",
       " 'yes': 612,\n",
       " ('in', 'the', 'film'): 613,\n",
       " ('the', 'director'): 614,\n",
       " 'against': 615,\n",
       " ('the', 'fact', 'that'): 616,\n",
       " 'entire': 617,\n",
       " ('in', 'this', 'movie'): 618,\n",
       " 'evil': 619,\n",
       " ('it', 'to'): 620,\n",
       " ('and', 'that'): 621,\n",
       " 'absolutely': 622,\n",
       " 'problem': 623,\n",
       " ('be', 'the'): 624,\n",
       " 'white': 625,\n",
       " 'went': 626,\n",
       " ('it', 'and'): 627,\n",
       " 'boy': 628,\n",
       " ('the', 'last'): 629,\n",
       " 'worse': 630,\n",
       " ('the', 'script'): 631,\n",
       " ('there', 'was'): 632,\n",
       " 'waste': 633,\n",
       " 'picture': 634,\n",
       " 'fans': 635,\n",
       " 'called': 636,\n",
       " ('of', 'her'): 637,\n",
       " ('and', 'is'): 638,\n",
       " ('so', 'much'): 639,\n",
       " 'overall': 640,\n",
       " 'cinema': 641,\n",
       " ('in', 'fact'): 642,\n",
       " ('at', 'a'): 643,\n",
       " ('had', 'a'): 644,\n",
       " ('see', 'the'): 645,\n",
       " ('it', 'a'): 646,\n",
       " 'loved': 647,\n",
       " 'certainly': 648,\n",
       " 'several': 649,\n",
       " 'hope': 650,\n",
       " ('of', 'my'): 651,\n",
       " 'lives': 652,\n",
       " 'mr': 653,\n",
       " 'entertaining': 654,\n",
       " ('it', 'is', 'a'): 655,\n",
       " ('in', 'the', 'movie'): 656,\n",
       " 'based': 657,\n",
       " ('he', 'has'): 658,\n",
       " 'under': 659,\n",
       " ('story', 'is'): 660,\n",
       " 'beginning': 661,\n",
       " 'mother': 662,\n",
       " ('the', 'show'): 663,\n",
       " 'guys': 664,\n",
       " ('to', 'find'): 665,\n",
       " 'drama': 666,\n",
       " 'direction': 667,\n",
       " 'throughout': 668,\n",
       " 'care': 669,\n",
       " ('about', 'this'): 670,\n",
       " ('supposed', 'to'): 671,\n",
       " ('film', 'was'): 672,\n",
       " ('will', 'be'): 673,\n",
       " 'oh': 674,\n",
       " ('sort', 'of'): 675,\n",
       " ('in', 'this', 'film'): 676,\n",
       " 'seemed': 677,\n",
       " 'example': 678,\n",
       " 'dark': 679,\n",
       " 'children': 680,\n",
       " ('and', 'he'): 681,\n",
       " 'becomes': 682,\n",
       " 'already': 683,\n",
       " ('does', 'not'): 684,\n",
       " 'laugh': 685,\n",
       " ('can', 'be'): 686,\n",
       " ('it', 'would'): 687,\n",
       " 'wanted': 688,\n",
       " 'killer': 689,\n",
       " 'friend': 690,\n",
       " '\\x96': 691,\n",
       " ('just', 'a'): 692,\n",
       " 'unfortunately': 693,\n",
       " ('and', 'its'): 694,\n",
       " 'sound': 695,\n",
       " 'final': 696,\n",
       " ('rest', 'of'): 697,\n",
       " 'turn': 698,\n",
       " 'writing': 699,\n",
       " ('is', 'no'): 700,\n",
       " 'despite': 701,\n",
       " ('movie', 'that'): 702,\n",
       " 'humor': 703,\n",
       " 'wants': 704,\n",
       " 'lead': 705,\n",
       " ('is', 'in'): 706,\n",
       " ('should', 'be'): 707,\n",
       " 'past': 708,\n",
       " ('the', 'cast'): 709,\n",
       " ('not', 'the'): 710,\n",
       " ('i', 'cant'): 711,\n",
       " ('film', 'that'): 712,\n",
       " 'son': 713,\n",
       " 'close': 714,\n",
       " 'days': 715,\n",
       " ('with', 'this'): 716,\n",
       " 'quality': 717,\n",
       " 'amazing': 718,\n",
       " 'totally': 719,\n",
       " ('not', 'to'): 720,\n",
       " 'michael': 721,\n",
       " 'heart': 722,\n",
       " 'girls': 723,\n",
       " 'able': 724,\n",
       " 'history': 725,\n",
       " ('the', 'rest', 'of'): 726,\n",
       " 'wo': 727,\n",
       " 'game': 728,\n",
       " 'guess': 729,\n",
       " ('to', 'this'): 730,\n",
       " ('the', 'book'): 731,\n",
       " 'child': 732,\n",
       " ('because', 'of'): 733,\n",
       " ('about', 'a'): 734,\n",
       " 'art': 735,\n",
       " 'fine': 736,\n",
       " ('hard', 'to'): 737,\n",
       " ('i', 'didnt'): 738,\n",
       " ('ever', 'seen'): 739,\n",
       " 'side': 740,\n",
       " ('and', 'this'): 741,\n",
       " ('i', 'could'): 742,\n",
       " 'tries': 743,\n",
       " ('the', 'audience'): 744,\n",
       " 'flick': 745,\n",
       " ('as', 'i'): 746,\n",
       " ('make', 'a'): 747,\n",
       " 'ill': 748,\n",
       " ('watch', 'it'): 749,\n",
       " 'behind': 750,\n",
       " 'enjoyed': 751,\n",
       " 'turns': 752,\n",
       " ('you', 'will'): 753,\n",
       " 'works': 754,\n",
       " ('to', 'go'): 755,\n",
       " 'hand': 756,\n",
       " ('it', 'the'): 757,\n",
       " ('such', 'as'): 758,\n",
       " ('i', 'found'): 759,\n",
       " ('most', 'of', 'the'): 760,\n",
       " 'favorite': 761,\n",
       " 'themselves': 762,\n",
       " '4': 763,\n",
       " ('to', 'his'): 764,\n",
       " ('when', 'he'): 765,\n",
       " ('up', 'to'): 766,\n",
       " ('able', 'to'): 767,\n",
       " 'act': 768,\n",
       " ('the', 'films'): 769,\n",
       " ('is', 'also'): 770,\n",
       " ('end', 'of'): 771,\n",
       " 'ones': 772,\n",
       " 'gave': 773,\n",
       " ('at', 'the', 'end'): 774,\n",
       " 'soon': 775,\n",
       " 'genre': 776,\n",
       " 'town': 777,\n",
       " 'car': 778,\n",
       " ('for', 'me'): 779,\n",
       " ('because', 'it'): 780,\n",
       " ('see', 'it'): 781,\n",
       " ('was', 'not'): 782,\n",
       " ('than', 'the'): 783,\n",
       " ('not', 'only'): 784,\n",
       " ('had', 'to'): 785,\n",
       " 'viewer': 786,\n",
       " 'kill': 787,\n",
       " ('it', 'i'): 788,\n",
       " 'starts': 789,\n",
       " 'eyes': 790,\n",
       " 'city': 791,\n",
       " 'run': 792,\n",
       " ('but', 'its'): 793,\n",
       " 'late': 794,\n",
       " 'sometimes': 795,\n",
       " ('acting', 'is'): 796,\n",
       " 'actress': 797,\n",
       " 'horrible': 798,\n",
       " ('that', 'you'): 799,\n",
       " ('watch', 'this'): 800,\n",
       " 'expect': 801,\n",
       " ('see', 'this'): 802,\n",
       " 'directed': 803,\n",
       " ('has', 'to'): 804,\n",
       " ('of', 'their'): 805,\n",
       " ('to', 'me'): 806,\n",
       " 'brilliant': 807,\n",
       " ('as', 'an'): 808,\n",
       " ('and', 'not'): 809,\n",
       " 'parts': 810,\n",
       " 'stories': 811,\n",
       " 'stuff': 812,\n",
       " ('this', 'is', 'the'): 813,\n",
       " 'blood': 814,\n",
       " 'thinking': 815,\n",
       " 'obviously': 816,\n",
       " ('should', 'have'): 817,\n",
       " ('after', 'the'): 818,\n",
       " ('well', 'as'): 819,\n",
       " 'self': 820,\n",
       " ('this', 'movie', 'was'): 821,\n",
       " 'slow': 822,\n",
       " ('up', 'with'): 823,\n",
       " ('back', 'to'): 824,\n",
       " ('so', 'i'): 825,\n",
       " ('so', 'many'): 826,\n",
       " 'matter': 827,\n",
       " ('on', 'this'): 828,\n",
       " ('of', 'those'): 829,\n",
       " 'etc': 830,\n",
       " ('where', 'the'): 831,\n",
       " 'hour': 832,\n",
       " 'feeling': 833,\n",
       " ('say', 'that'): 834,\n",
       " ('i', 'really'): 835,\n",
       " 'decent': 836,\n",
       " 'voice': 837,\n",
       " ('have', 'seen'): 838,\n",
       " ('did', 'not'): 839,\n",
       " 'heard': 840,\n",
       " ('movie', 'the'): 841,\n",
       " 'kid': 842,\n",
       " 'fight': 843,\n",
       " ('very', 'good'): 844,\n",
       " ('seem', 'to'): 845,\n",
       " ('are', 'the'): 846,\n",
       " ('do', 'not'): 847,\n",
       " ('a', 'bad'): 848,\n",
       " 'myself': 849,\n",
       " ('film', 'the'): 850,\n",
       " ('enough', 'to'): 851,\n",
       " 'hit': 852,\n",
       " ('film', 'i'): 853,\n",
       " ('in', 'their'): 854,\n",
       " 'stop': 855,\n",
       " ('the', 'ending'): 856,\n",
       " 'highly': 857,\n",
       " 'took': 858,\n",
       " ('i', 'just'): 859,\n",
       " ('in', 'which'): 860,\n",
       " ('each', 'other'): 861,\n",
       " 'type': 862,\n",
       " ('i', 'know'): 863,\n",
       " 'god': 864,\n",
       " 'except': 865,\n",
       " 'killed': 866,\n",
       " ('the', 'one'): 867,\n",
       " ('you', 'want'): 868,\n",
       " 'writer': 869,\n",
       " 'hell': 870,\n",
       " ('ive', 'seen'): 871,\n",
       " 'roles': 872,\n",
       " ('during', 'the'): 873,\n",
       " ('in', 'an'): 874,\n",
       " ('to', 'take'): 875,\n",
       " 'says': 876,\n",
       " ('the', 'entire'): 877,\n",
       " 'age': 878,\n",
       " 'obvious': 879,\n",
       " ('special', 'effects'): 880,\n",
       " 'happened': 881,\n",
       " ('a', 'big'): 882,\n",
       " '5': 883,\n",
       " 'violence': 884,\n",
       " ('and', 'they'): 885,\n",
       " 'police': 886,\n",
       " 'involved': 887,\n",
       " ('better', 'than'): 888,\n",
       " 'strong': 889,\n",
       " 'moment': 890,\n",
       " ('what', 'i'): 891,\n",
       " 'leave': 892,\n",
       " 'happens': 893,\n",
       " ('are', 'a'): 894,\n",
       " ('of', 'an'): 895,\n",
       " 'james': 896,\n",
       " ('was', 'so'): 897,\n",
       " 'particularly': 898,\n",
       " 'alone': 899,\n",
       " ('are', 'not'): 900,\n",
       " ('about', 'it'): 901,\n",
       " 'living': 902,\n",
       " 'known': 903,\n",
       " ('piece', 'of'): 904,\n",
       " ('with', 'her'): 905,\n",
       " ('and', 'in'): 906,\n",
       " ('it', 'all'): 907,\n",
       " ('in', 'all'): 908,\n",
       " 'happen': 909,\n",
       " 'chance': 910,\n",
       " 'extremely': 911,\n",
       " 'coming': 912,\n",
       " 'simple': 913,\n",
       " ('in', 'it'): 914,\n",
       " 'told': 915,\n",
       " 'murder': 916,\n",
       " ('a', 'couple'): 917,\n",
       " ('what', 'the'): 918,\n",
       " ('when', 'it'): 919,\n",
       " ('this', 'show'): 920,\n",
       " 'b': 921,\n",
       " ('played', 'by'): 922,\n",
       " ('tries', 'to'): 923,\n",
       " ('is', 'about'): 924,\n",
       " ('the', 'end', 'of'): 925,\n",
       " 'anyway': 926,\n",
       " ('of', 'the', 'most'): 927,\n",
       " 'including': 928,\n",
       " ('rest', 'of', 'the'): 929,\n",
       " 'gore': 930,\n",
       " ('if', 'the'): 931,\n",
       " ('story', 'of'): 932,\n",
       " 'cut': 933,\n",
       " ('as', 'well', 'as'): 934,\n",
       " 'wonder': 935,\n",
       " 'brother': 936,\n",
       " 'ago': 937,\n",
       " 'experience': 938,\n",
       " 'attempt': 939,\n",
       " ('and', 'even'): 940,\n",
       " 'group': 941,\n",
       " 'ok': 942,\n",
       " ('too', 'much'): 943,\n",
       " 'daughter': 944,\n",
       " 'save': 945,\n",
       " ('based', 'on'): 946,\n",
       " 'hero': 947,\n",
       " 'complete': 948,\n",
       " ('there', 'is', 'a'): 949,\n",
       " 'career': 950,\n",
       " 'lack': 951,\n",
       " ('a', 'real'): 952,\n",
       " 'score': 953,\n",
       " ('she', 'was'): 954,\n",
       " ('as', 'he'): 955,\n",
       " ('a', 'man'): 956,\n",
       " ('out', 'the'): 957,\n",
       " ('dont', 'know'): 958,\n",
       " ('what', 'is'): 959,\n",
       " 'exactly': 960,\n",
       " 'released': 961,\n",
       " ('even', 'though'): 962,\n",
       " 'crap': 963,\n",
       " 'none': 964,\n",
       " ('i', 'love'): 965,\n",
       " 'lets': 966,\n",
       " 'looked': 967,\n",
       " ('out', 'of', 'the'): 968,\n",
       " 'number': 969,\n",
       " ('people', 'who'): 970,\n",
       " ('if', 'it'): 971,\n",
       " ('it', 'was', 'a'): 972,\n",
       " 'please': 973,\n",
       " 'interest': 974,\n",
       " ('i', 'mean'): 975,\n",
       " 'sad': 976,\n",
       " 'whose': 977,\n",
       " ('instead', 'of'): 978,\n",
       " 'serious': 979,\n",
       " 'across': 980,\n",
       " 'seriously': 981,\n",
       " ('of', 'its'): 982,\n",
       " 'ends': 983,\n",
       " 'ridiculous': 984,\n",
       " ('and', 'you'): 985,\n",
       " 'power': 986,\n",
       " 'yourself': 987,\n",
       " ('the', 'very'): 988,\n",
       " ('the', 'music'): 989,\n",
       " ('the', 'story', 'is'): 990,\n",
       " 'possible': 991,\n",
       " ('but', 'not'): 992,\n",
       " 'taken': 993,\n",
       " 'running': 994,\n",
       " 'david': 995,\n",
       " ('who', 'has'): 996,\n",
       " 'song': 997,\n",
       " 'today': 998,\n",
       " ('try', 'to'): 999,\n",
       " 'hilarious': 1000,\n",
       " 'english': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 9466 ; token ('here', 'are')\n",
      "Token ('here', 'are'); token id 9466\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary \n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens_quagram)\n",
    "val_data_indices = token2index_dataset(val_data_tokens_quagram)\n",
    "test_data_indices = token2index_dataset(test_data_tokens_quagram)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build PyTorch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100 # bigger is better, 200, 500...\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 73.76\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 81.88\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 83.68\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 85.74\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 85.24\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 85.8\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 86.04\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.32\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 85.84\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 85.78\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 83.56\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 85.34\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 85.56\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 85.24\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 85.68\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 85.84\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 85.24\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 85.36\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 84.22\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 84.66\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 84.62\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 84.56\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 84.82\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 84.9\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 84.72\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 84.44\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 84.36\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 84.22\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 84.62\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 84.06\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 84.12\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 83.66\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 83.66\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 84.08\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 83.72\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 83.58\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 83.34\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 83.62\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 83.38\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 82.82\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 81.48\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 82.86\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 82.96\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 82.96\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 82.46\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 82.66\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 83.04\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 83.18\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 82.72\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 82.68\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 82.5\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 82.62\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 82.44\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 82.48\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 82.44\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 82.06\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 82.7\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 82.1\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 82.48\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 82.36\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 82.72\n",
      "Test Acc 80.36\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "2-gram Val Acc 80.4\n",
      "2-gram Test Acc 80.06\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"2-gram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "3-gram Val Acc 75.38\n",
      "3-gram Test Acc 74.892\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"3-gram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"4-gram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "bigram Val Acc 81.48\n",
      "bigram Test Acc 80.076\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"bigram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "trigram Val Acc 82.3\n",
      "trigram Test Acc 81.232\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"trigram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "fourrgram Val Acc 82.0\n",
      "fourgram Test Acc 81.256\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"fourrgram Val Acc {}\".format(test_model(val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save fourgram tokens for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(val_data_tokens_trigram, open(\"val_data_tokens_3gram.p\", \"wb\"))\n",
    "pkl.dump(test_data_tokens_trigram, open(\"test_data_tokens_3gram.p\", \"wb\"))\n",
    "pkl.dump(train_data_tokens_trigram, open(\"train_data_tokens_3gram.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens_trigram, open(\"all_train_tokens_3gram.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(val_label, open(\"val_label.p\", \"wb\"))\n",
    "pkl.dump(test_label, open(\"test_label.p\", \"wb\"))\n",
    "pkl.dump(train_label, open(\"train_label.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a great movie some will disagree with me but  if you know anything about the bible you know it is i think everyone should see it i agree a new updated version like be nice but the message is still right on if you can see this movie is not a scare the hell of you movieit is truthful with the bible i think the u n will play a major role in the world government to come the last days are lining up with the bible look at what has happened with the chip for dogs and cats that now has come to light to protect on children from being kidnapped its the size of a grain of rice this i feel is the fore runner of the mark of the beast spoken of in the bible without the mark you cant sell or buy with this chip that small in the future there is no telling how much info can be put on it'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i saw only the first part of this series when it debuted back in the late 90s and only recently got a chance to watch all three parts via netflix convenient service by the way all in all i liked this lighthearted sometimes genre challenged mini series the story of a younger man falling for an older woman seems to work and the actors are all fine yes it does have some romance clichés of running in the rain or a train station goodbye but the characters have a chance to be explored so it doesnt seem cheesy like it would be if this were some tom hanks vehicle or similar robson greene who at times reminds me of a separated at birth scott bakula does a fine job of someone who is head over heels in love and the ebb and tide of desire and rejection throws the series into watchable fare personally i think the series could have been done with two episodes but thats up for debate i suppose apparently theres a sequel and that should be arriving tomorrow via netflix'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this columbo episode is probably noted more for the director steven spielberg as one of his early films it should be looked at for jack cassidys role as the murderer who kills his partner in writing to maintain his lifestyle jack cassidy would appear in a later columbo after all columbo meets his match in jack cassidys character he is a mystery writer who plots to perform the perfect murder after his first murder his next victim would be the annoying general store owner widow who would blackmail him for money rather than losing more money he kills her it is very entertaining to watch cassidy and falk as always falks familiarity as columbo makes him watchable after viewing this episode repeatedly over the years what television today forgets about the success of years is that people will want to watch the shows again and again if they like the characters its not about who does it how and why it the familiarness of columbo and his likability which scores high with viewers like myself'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twisted desire 1996 was a tv movie starring melissa joan hart melissas character jennifer stanton a seventeen year old seduces her current boyfriend nick ryan into murdering her two parents the movie is based on the 1990 murders of the parents of 14 year old jessica wiseman jessica had her 17 year old boyfriend douglas christopher thomas shoot and kill her parents thomas was executed in 2000 jessica was released from prison when she turned 21 years old evidence now suggests that it was jessica who fired the fatal shot that killed her mother jessica is known to now be residing somewhere in the state of virginia'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
