{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda = torch.device('cuda')\n",
    "else:\n",
    "    cuda = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrain = pd.read_csv('hw2_data/mnli_train.tsv', delimiter='\\t')\n",
    "mval = pd.read_csv('hw2_data/mnli_val.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and now that was in fifty one that 's forty ye...</td>\n",
       "      <td>It was already a problem forty years ago but n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jon could smell baked bread on the air and his...</td>\n",
       "      <td>Jon smelt food in the air and was hungry .</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it will be like Italian basketball with the uh...</td>\n",
       "      <td>This type of Italian basketball is nothing lik...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>well i think that 's about uh that 's about co...</td>\n",
       "      <td>Sorry but we are not done just yet .</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good job tenure , that is -- because in yet an...</td>\n",
       "      <td>Dr. Quinn , Medicine Woman , was worked on by ...</td>\n",
       "      <td>entailment</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  and now that was in fifty one that 's forty ye...   \n",
       "1  Jon could smell baked bread on the air and his...   \n",
       "2  it will be like Italian basketball with the uh...   \n",
       "3  well i think that 's about uh that 's about co...   \n",
       "4  Good job tenure , that is -- because in yet an...   \n",
       "\n",
       "                                           sentence2          label      genre  \n",
       "0  It was already a problem forty years ago but n...        neutral  telephone  \n",
       "1         Jon smelt food in the air and was hungry .        neutral    fiction  \n",
       "2  This type of Italian basketball is nothing lik...  contradiction  telephone  \n",
       "3               Sorry but we are not done just yet .  contradiction  telephone  \n",
       "4  Dr. Quinn , Medicine Woman , was worked on by ...     entailment      slate  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fiction', 'government', 'slate', 'telephone', 'travel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(mtrain['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fiction</th>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>1016</td>\n",
       "      <td>1016</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slate</th>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telephone</th>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>982</td>\n",
       "      <td>982</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentence1  sentence2  label\n",
       "genre                                  \n",
       "fiction           995        995    995\n",
       "government       1016       1016   1016\n",
       "slate            1002       1002   1002\n",
       "telephone        1005       1005   1005\n",
       "travel            982        982    982"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mval.groupby(['genre']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s1, train_s2, train_label, train_genre = np.array(mtrain['sentence1']), np.array(mtrain['sentence2']), np.array(mtrain['label']), np.array(mtrain['genre'])\n",
    "val_s1, val_s2, val_label, val_genre = np.array(mval['sentence1']), np.array(mval['sentence2']), np.array(mval['label']), np.array(mval['genre'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"neutral\":0, \"entailment\":1, 'contradiction':2}\n",
    "train_nlabel = []\n",
    "val_nlabel = []\n",
    "\n",
    "for l in train_label:\n",
    "    train_nlabel.append(label_map[l])\n",
    "    \n",
    "for v in val_label:\n",
    "    val_nlabel.append(label_map[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ngram_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        ngram = sample.split()\n",
    "        tokens = []\n",
    "        for gram in ngram:\n",
    "            tokens.append(gram)\n",
    "        \n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "train_data_s1_token, all_train_tokens_s1 = tokenize_ngram_dataset(train_s1)\n",
    "train_data_s2_token, all_train_tokens_s2 = tokenize_ngram_dataset(train_s2)\n",
    "all_train_tokens = all_train_tokens_s1 + all_train_tokens_s2\n",
    "val_data_s1_token, _ = tokenize_ngram_dataset(val_s1)\n",
    "val_data_s2_token, _ = tokenize_ngram_dataset(val_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "token_id = {}\n",
    "for i, word in enumerate(id2token):\n",
    "    token_id[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_s1_indices = token2index_dataset(train_data_s1_token)\n",
    "train_s2_indices = token2index_dataset(train_data_s2_token)\n",
    "val_s1_indices = token2index_dataset(val_data_s1_token)\n",
    "val_s2_indices = token2index_dataset(val_data_s2_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "class SenDataLoader(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data1, data2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data1 = data1\n",
    "        self.data2 = data2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data1) == len(self.target_list))\n",
    "        assert (len(self.data2) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        s1_idx = self.data1[key][:MAX_SENTENCE_LENGTH]\n",
    "        s2_idx = self.data2[key][:MAX_SENTENCE_LENGTH]\n",
    "        \n",
    "        label = self.target_list[key]\n",
    "        \n",
    "        return [s1_idx, s2_idx, len(s1_idx), len(s2_idx), label]\n",
    "\n",
    "def sen_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "    s1_length = []\n",
    "    s2_length = []\n",
    "    label_list = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        s1_length.append(datum[2])\n",
    "        s2_length.append(datum[3])\n",
    "        label_list.append(datum[4])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec_s1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec_s2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data1.append(padded_vec_s1)\n",
    "        data2.append(padded_vec_s2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data1)), torch.from_numpy(np.array(data2)), torch.LongTensor(s1_length), torch.LongTensor(s2_length), torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fiction_index = [i for i, x in enumerate(list(train_genre)) if x == \"fiction\"]\n",
    "train_government_index = [i for i, x in enumerate(list(train_genre)) if x == \"government\"]\n",
    "train_slate_index = [i for i, x in enumerate(list(train_genre)) if x == \"slate\"]\n",
    "train_telephone_index = [i for i, x in enumerate(list(train_genre)) if x == \"telephone\"]\n",
    "train_travel_index = [i for i, x in enumerate(list(train_genre)) if x == \"travel\"]\n",
    "val_fiction_index = [i for i, x in enumerate(list(val_genre)) if x == \"fiction\"]\n",
    "val_government_index = [i for i, x in enumerate(list(val_genre)) if x == \"government\"]\n",
    "val_slate_index = [i for i, x in enumerate(list(val_genre)) if x == \"slate\"]\n",
    "val_telephone_index = [i for i, x in enumerate(list(val_genre)) if x == \"telephone\"]\n",
    "val_travel_index = [i for i, x in enumerate(list(val_genre)) if x == \"travel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_index(indices1, indices2, index):\n",
    "    new_indices1 = []\n",
    "    new_indices2 = []\n",
    "    for i in index:\n",
    "        new_indices1.append(indices1[i])\n",
    "        new_indices2.append(indices2[i])\n",
    "    return new_indices1, new_indices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fiction_s1_indices, train_fiction_s2_indices = indices_index(train_s1_indices, train_s2_indices, train_fiction_index)\n",
    "train_gov_s1_indices, train_gov_s2_indices = indices_index(train_s1_indices, train_s2_indices, train_government_index)\n",
    "train_slate_s1_indices, train_slate_s2_indices = indices_index(train_s1_indices, train_s2_indices, train_slate_index)\n",
    "train_tel_s1_indices, train_tel_s2_indices = indices_index(train_s1_indices, train_s2_indices, train_telephone_index)\n",
    "train_travel_s1_indices, train_travel_s2_indices = indices_index(train_s1_indices, train_s2_indices, train_travel_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fiction_s1_indices, val_fiction_s2_indices = indices_index(val_s1_indices, val_s2_indices, val_fiction_index)\n",
    "val_gov_s1_indices, val_gov_s2_indices = indices_index(val_s1_indices, val_s2_indices, val_government_index)\n",
    "val_slate_s1_indices, val_slate_s2_indices = indices_index(val_s1_indices, val_s2_indices, val_slate_index)\n",
    "val_tel_s1_indices, val_tel_s2_indices = indices_index(val_s1_indices, val_s2_indices, val_telephone_index)\n",
    "val_travel_s1_indices, val_travel_s2_indices = indices_index(val_s1_indices, val_s2_indices, val_travel_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fiction_label = [train_nlabel[i] for i in train_fiction_index]\n",
    "train_gov_label = [train_nlabel[i] for i in train_government_index]\n",
    "train_slate_label = [train_nlabel[i] for i in train_slate_index]\n",
    "train_tel_label = [train_nlabel[i] for i in train_telephone_index]\n",
    "train_travel_label = [train_nlabel[i] for i in train_travel_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fiction_label = [val_nlabel[i] for i in val_fiction_index]\n",
    "val_gov_label = [val_nlabel[i] for i in val_government_index]\n",
    "val_slate_label = [val_nlabel[i] for i in val_slate_index]\n",
    "val_tel_label = [val_nlabel[i] for i in val_telephone_index]\n",
    "val_travel_label = [val_nlabel[i] for i in val_travel_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fiction = SenDataLoader(train_fiction_s1_indices, train_fiction_s2_indices, train_fiction_label)\n",
    "train_loader_fiction = torch.utils.data.DataLoader(dataset=train_fiction, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "train_gov = SenDataLoader(train_gov_s1_indices, train_gov_s2_indices, train_gov_label)\n",
    "train_loader_gov = torch.utils.data.DataLoader(dataset=train_gov, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "train_slate = SenDataLoader(train_slate_s1_indices, train_slate_s2_indices, train_slate_label)\n",
    "train_loader_slate = torch.utils.data.DataLoader(dataset=train_slate, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "train_tel = SenDataLoader(train_tel_s1_indices, train_tel_s2_indices, train_tel_label)\n",
    "train_loader_tel = torch.utils.data.DataLoader(dataset=train_tel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "train_travel = SenDataLoader(train_travel_s1_indices, train_travel_s2_indices, train_travel_label)\n",
    "train_loader_travel = torch.utils.data.DataLoader(dataset=train_travel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_fiction = SenDataLoader(val_fiction_s1_indices, val_fiction_s2_indices, val_fiction_label)\n",
    "val_loader_fiction = torch.utils.data.DataLoader(dataset=val_fiction, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_gov = SenDataLoader(val_gov_s1_indices, val_gov_s2_indices, val_gov_label)\n",
    "val_loader_gov = torch.utils.data.DataLoader(dataset=val_gov, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_slate = SenDataLoader(val_slate_s1_indices, val_slate_s2_indices, val_slate_label)\n",
    "val_loader_slate = torch.utils.data.DataLoader(dataset=val_slate, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_tel = SenDataLoader(val_tel_s1_indices, val_tel_s2_indices, val_tel_label)\n",
    "val_loader_tel = torch.utils.data.DataLoader(dataset=val_tel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_travel = SenDataLoader(val_travel_s1_indices, val_travel_s2_indices, val_travel_label)\n",
    "val_loader_travel = torch.utils.data.DataLoader(dataset=val_travel, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=sen_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 200])\n",
      "tensor([11, 14,  5,  5, 20, 23, 18,  9,  6, 10,  4, 12,  9,  4, 20,  4, 11, 18,\n",
      "         9,  6, 15, 28, 19, 10,  7,  3,  9, 10,  6, 21, 18, 11])\n",
      "tensor([ 9, 13,  8,  5, 12,  5, 17, 14,  9, 10,  3, 10,  8,  6, 10, 13,  9, 13,\n",
      "        10,  7,  7, 12, 12,  8,  6,  4,  7,  9,  9, 10, 14, 16])\n",
      "tensor([1, 2, 0, 2, 0, 0, 2, 2, 1, 1, 1, 1, 0, 2, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2,\n",
      "        1, 2, 2, 0, 1, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, (s1, s2, length1, length2, label) in enumerate(val_loader_fiction):\n",
    "    print(s1.shape)\n",
    "    print(length1)\n",
    "    print(length2)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(tokens[1:]).astype(float)\n",
    "    return data\n",
    "\n",
    "def load_weight_matrix(vocab, pretrained_emb):\n",
    "    matrix_len = len(vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, 300))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = pretrained_emb[word]\n",
    "            words_found += 1\n",
    "            \n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.rand(300, ) - 0.5\n",
    "    \n",
    "    return weights_matrix\n",
    "\n",
    "pretrained_emb = load_vectors('wiki-news-300d-1M.vec')\n",
    "weights_matrix = load_weight_matrix(token_id, pretrained_emb)\n",
    "weightsmatrix = Variable(torch.Tensor(weights_matrix), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on validation set\n",
    "\n",
    "#### Best CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for s1, s2, length1, length2, labels in loader:\n",
    "        outputs = model(s1.to(device=cuda).long(), s2.to(device=cuda).long(), length1.to(device=cuda).long())\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.to(device=cuda).size(0)\n",
    "        correct += predicted.eq(labels.to(device=cuda).view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, fc_classes, num_classes, vocab_size, weights):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.weights = Variable(torch.Tensor(weights), requires_grad=False)\n",
    "        self.fc_classes = fc_classes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # s1\n",
    "        self.embedding1 = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX, _weight=weights)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # s2\n",
    "        self.embedding2 = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX, _weight=weights)\n",
    "    \n",
    "        self.conv3 = nn.Conv1d(emb_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.conv4 = nn.Conv1d(hidden_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size, fc_classes)\n",
    "        self.linear2 = nn.Linear(fc_classes, num_classes)\n",
    "\n",
    "    def forward(self, s1, s2, length1):\n",
    "        batch_size, seq_len = s1.size()\n",
    "\n",
    "        # CNN for s1\n",
    "        embed1 = self.embedding1(s1)\n",
    "        hidden1 = self.conv1(embed1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len, hidden1.size(-1))\n",
    "        hidden1 = self.conv2(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        hidden1 = F.relu(hidden1.contiguous().view(-1, hidden1.size(-1))).view(batch_size, seq_len, hidden1.size(-1))\n",
    "        hidden1 = self.pool1(hidden1.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # CNN for s2\n",
    "        embed2 = self.embedding2(s2)\n",
    "        hidden2 = self.conv3(embed2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "        hidden2 = self.conv4(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden2.size(-1))\n",
    "        hidden2 = self.pool2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        \n",
    "        # concat\n",
    "        hidden = torch.cat((hidden1, hidden2), 1)\n",
    "        \n",
    "        # fully connected layer\n",
    "        hidden = torch.sum(hidden, dim=1)\n",
    "        \n",
    "        fc1 = F.leaky_relu(self.linear1(hidden))\n",
    "        fc2 = self.linear2(fc1)\n",
    "        \n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = pkl.load(open(\"cnn_kernel5.p\", \"rb\"))\n",
    "torch.save(cnn_model.state_dict(),\"cnnmodel\")\n",
    "best_cnn = CNN(emb_size=300, hidden_size=128, num_layers=2, fc_classes=48, num_classes=3, vocab_size=len(id2token), weights=weightsmatrix)\n",
    "best_cnn.load_state_dict(torch.load('cnnmodel'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding1): Embedding(10002, 300, padding_idx=0)\n",
       "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (embedding2): Embedding(10002, 300, padding_idx=0)\n",
       "  (conv3): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv4): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Linear(in_features=128, out_features=48, bias=True)\n",
       "  (linear2): Linear(in_features=48, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding1): Embedding(10002, 300, padding_idx=0)\n",
       "  (conv1): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (embedding2): Embedding(10002, 300, padding_idx=0)\n",
       "  (conv3): Conv1d(300, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (conv4): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear1): Linear(in_features=128, out_features=48, bias=True)\n",
       "  (linear2): Linear(in_features=48, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cnn.to(device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = ['fiction', 'government', 'slate', 'telephone', 'travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, (s1, s2, length1, length2, label) in enumerate(val_loader_fiction):\n",
    "    outputs = best_cnn(s1.to(device=cuda).long(), s2.to(device=cuda).long(), length1.to(device=cuda).long())\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    predicted = outputs.max(1, keepdim=True)[1]\n",
    "    correct = predicted.eq(label.to(device=cuda).view_as(predicted))\n",
    "    total += label.to(device=cuda).size(0)\n",
    "    correct += predicted.eq(label.to(device=cuda).view_as(predicted)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_acc1 = test_model(val_loader_fiction, best_cnn)\n",
    "cnn_acc2 = test_model(val_loader_gov, best_cnn)\n",
    "cnn_acc3 = test_model(val_loader_slate, best_cnn)\n",
    "cnn_acc4 = test_model(val_loader_tel, best_cnn)\n",
    "cnn_acc5 = test_model(val_loader_travel, best_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_acc = [cnn_acc1, cnn_acc2, cnn_acc3, cnn_acc4, cnn_acc5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>CNN Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiction</td>\n",
       "      <td>35.879397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>government</td>\n",
       "      <td>31.496063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>32.335329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>telephone</td>\n",
       "      <td>35.024876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>34.012220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre    CNN Acc\n",
       "0     fiction  35.879397\n",
       "1  government  31.496063\n",
       "2       slate  32.335329\n",
       "3   telephone  35.024876\n",
       "4      travel  34.012220"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_mnli = pd.DataFrame(list(zip(genre, cnn_acc)), columns=['genre','CNN Acc'])\n",
    "cnn_mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, fc_classes, num_classes, vocab_size, weights, dropout):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.weights = Variable(torch.Tensor(weights), requires_grad=False)\n",
    "        self.fc_classes = fc_classes\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        # s1\n",
    "        self.embedding_s1 = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX, _weight=weights)\n",
    "        self.rnn_s1 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #s2\n",
    "        self.embedding_s2 = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX, _weight=weights)\n",
    "        self.rnn_s2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # fully connected\n",
    "        self.linear1 = nn.Linear(hidden_size * 2, fc_classes)\n",
    "        self.linear2 = nn.Linear(fc_classes, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden_s1 = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "        hidden_s2 = torch.randn(self.num_layers * 2, batch_size, self.hidden_size)\n",
    "\n",
    "        return hidden_s1.to(device=cuda), hidden_s2.to(device=cuda)\n",
    "\n",
    "    def forward(self, s1, s2, length1):\n",
    "        batch_size, seq_len = s1.size()\n",
    "\n",
    "        self.hidden_s1, self.hidden_s2 = self.init_hidden(batch_size)\n",
    "\n",
    "        # get embedding of characters\n",
    "        embed1 = self.embedding_s1(s1)\n",
    "        embed2 = self.embedding_s2(s2)\n",
    "        embed1.to(device=cuda)\n",
    "        embed2.to(device=cuda)\n",
    "        # s1\n",
    "        rnn_out_s1, hidden1 = self.rnn_s1(embed1, self.hidden_s1)\n",
    "        # s2\n",
    "        rnn_out_s2, hidden2 = self.rnn_s2(embed2, self.hidden_s2)\n",
    "        \n",
    "        \n",
    "        # concat\n",
    "        hidden = torch.cat((hidden1[1,:,:], hidden2[1,:,:]), 1)\n",
    "\n",
    "        fc1 = F.leaky_relu(self.linear1(hidden))\n",
    "        fc2 = self.linear2(fc1)\n",
    "        \n",
    "        return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (embedding_s1): Embedding(10002, 300, padding_idx=0)\n",
       "  (rnn_s1): GRU(300, 128, batch_first=True, bidirectional=True)\n",
       "  (embedding_s2): Embedding(10002, 300, padding_idx=0)\n",
       "  (rnn_s2): GRU(300, 128, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=256, out_features=48, bias=True)\n",
       "  (linear2): Linear(in_features=48, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsmatrix = Variable(torch.Tensor(weights_matrix), requires_grad=False)\n",
    "best_rnn = GRU(emb_size=300, hidden_size=128, num_layers=1, fc_classes=48, num_classes=3, vocab_size=len(id2token), weights=weightsmatrix, dropout=0.1)\n",
    "best_rnn.load_state_dict(torch.load('rnn_dropout'))\n",
    "best_rnn.eval()\n",
    "best_rnn.to(device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>RNN Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fiction</td>\n",
       "      <td>37.386935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>government</td>\n",
       "      <td>33.858268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slate</td>\n",
       "      <td>34.231537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>telephone</td>\n",
       "      <td>37.114428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>travel</td>\n",
       "      <td>36.150713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        genre    RNN Acc\n",
       "0     fiction  37.386935\n",
       "1  government  33.858268\n",
       "2       slate  34.231537\n",
       "3   telephone  37.114428\n",
       "4      travel  36.150713"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_acc1 = test_model(val_loader_fiction, best_rnn)\n",
    "rnn_acc2 = test_model(val_loader_gov, best_rnn)\n",
    "rnn_acc3 = test_model(val_loader_slate, best_rnn)\n",
    "rnn_acc4 = test_model(val_loader_tel, best_rnn)\n",
    "rnn_acc5 = test_model(val_loader_travel, best_rnn)\n",
    "rnn_acc = [rnn_acc1, rnn_acc2, rnn_acc3, rnn_acc4, rnn_acc5]\n",
    "rnn_mnli = pd.DataFrame(list(zip(genre, rnn_acc)), columns=['genre','RNN Acc'])\n",
    "rnn_mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
